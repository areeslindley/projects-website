{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning & Imputation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Missing data and inconsistencies require careful handling to avoid bias in downstream modeling. This notebook documents our data cleaning strategy, including imputation methods, outlier detection, and feature engineering.\n",
        "\n",
        "## Navigation\n",
        "\n",
        "- **Previous**: [Data Exploration](01_exploration.ipynb)\n",
        "- **Next**: [Modeling](03_modeling.ipynb)\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Develop and implement missing data imputation strategies\n",
        "2. Detect and handle outliers appropriately\n",
        "3. Engineer features for modeling\n",
        "4. Encode categorical variables\n",
        "5. Validate transformations and ensure no data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn for preprocessing\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_context(\"notebook\", font_scale=1.1)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Raw Data\n",
        "\n",
        "We'll start from the raw dataset and apply all cleaning steps systematically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    raw_data = pd.read_csv('data/titanic.csv')\n",
        "    print(f\"Dataset loaded: {raw_data.shape[0]} rows, {raw_data.shape[1]} columns\")\n",
        "    \n",
        "    # Create a working copy\n",
        "    data = raw_data.copy()\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"Error: titanic.csv not found in data/ directory\")\n",
        "    data = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering: Extract Title\n",
        "\n",
        "Before imputing Age, we'll extract titles from names as they correlate strongly with both age and survival."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Extract title from name\n",
        "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    \n",
        "    # Consolidate rare titles\n",
        "    title_mapping = {\n",
        "        'Mr': 'Mr',\n",
        "        'Miss': 'Miss',\n",
        "        'Mrs': 'Mrs',\n",
        "        'Master': 'Master',\n",
        "        'Dr': 'Rare',\n",
        "        'Rev': 'Rare',\n",
        "        'Col': 'Rare',\n",
        "        'Major': 'Rare',\n",
        "        'Mlle': 'Miss',\n",
        "        'Countess': 'Rare',\n",
        "        'Ms': 'Miss',\n",
        "        'Lady': 'Rare',\n",
        "        'Jonkheer': 'Rare',\n",
        "        'Don': 'Rare',\n",
        "        'Dona': 'Rare',\n",
        "        'Mme': 'Mrs',\n",
        "        'Capt': 'Rare',\n",
        "        'Sir': 'Rare'\n",
        "    }\n",
        "    data['Title'] = data['Title'].map(title_mapping)\n",
        "    data['Title'] = data['Title'].fillna('Rare')\n",
        "    \n",
        "    print(\"Title extraction complete:\")\n",
        "    print(data['Title'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Data Imputation Strategy\n",
        "\n",
        "### Age Imputation\n",
        "\n",
        "Age has ~20% missing values. We'll use median imputation grouped by Pclass and Title, as these are strong predictors of age (e.g., \"Master\" indicates children, \"Mr\" indicates adults)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Check missing Age by groups\n",
        "    print(\"Missing Age by Pclass and Title:\")\n",
        "    print(\"=\"*50)\n",
        "    missing_age = data.groupby(['Pclass', 'Title'])['Age'].agg(['count', 'size'])\n",
        "    missing_age['missing'] = missing_age['size'] - missing_age['count']\n",
        "    missing_age['missing_pct'] = (missing_age['missing'] / missing_age['size']) * 100\n",
        "    display(missing_age[missing_age['missing'] > 0])\n",
        "    \n",
        "    # Impute Age using median by Pclass and Title\n",
        "    # This approach preserves the relationship between age, class, and title\n",
        "    data['Age'] = data.groupby(['Pclass', 'Title'])['Age'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    \n",
        "    # If any remain (shouldn't happen), use overall median\n",
        "    data['Age'] = data['Age'].fillna(data['Age'].median())\n",
        "    \n",
        "    print(f\"\\nAge imputation complete. Missing values: {data['Age'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embarked Imputation\n",
        "\n",
        "Only 2 missing values - we'll use mode imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Impute Embarked with mode\n",
        "    mode_embarked = data['Embarked'].mode()[0]\n",
        "    data['Embarked'] = data['Embarked'].fillna(mode_embarked)\n",
        "    \n",
        "    print(f\"Embarked imputation complete. Missing values: {data['Embarked'].isnull().sum()}\")\n",
        "    print(f\"Mode used: {mode_embarked}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fare Imputation\n",
        "\n",
        "Single missing value - impute with median of same Pclass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Impute Fare with median by Pclass\n",
        "    data['Fare'] = data.groupby('Pclass')['Fare'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    \n",
        "    print(f\"Fare imputation complete. Missing values: {data['Fare'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cabin Treatment\n",
        "\n",
        "Cabin has ~77% missing values. Rather than imputing, we'll create a binary \"Cabin Known\" feature, which may be informative (passengers with known cabins might have been closer to lifeboats or had higher status)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Create binary feature for cabin\n",
        "    data['HasCabin'] = data['Cabin'].notna().astype(int)\n",
        "    \n",
        "    # Check survival rate by cabin status\n",
        "    cabin_survival = data.groupby('HasCabin')['Survived'].mean()\n",
        "    print(\"Survival rate by cabin status:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"No Cabin: {cabin_survival[0]:.2%}\")\n",
        "    print(f\"Has Cabin: {cabin_survival[1]:.2%}\")\n",
        "    \n",
        "    # Drop original Cabin column\n",
        "    data = data.drop('Cabin', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Fare Outlier Detection](images/fare_outliers.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Detection\n",
        "\n",
        "Let's identify potential outliers, particularly in Fare, which showed high variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # IQR-based outlier detection for Fare\n",
        "    Q1 = data['Fare'].quantile(0.25)\n",
        "    Q3 = data['Fare'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = data[(data['Fare'] < lower_bound) | (data['Fare'] > upper_bound)]\n",
        "    \n",
        "    print(f\"Fare Outlier Detection (IQR method):\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
        "    print(f\"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
        "    print(f\"Number of outliers: {len(outliers)} ({len(outliers)/len(data)*100:.1f}%)\")\n",
        "    \n",
        "    # Visualize outliers\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.boxplot(data['Fare'])\n",
        "    ax.set_ylabel('Fare')\n",
        "    ax.set_title('Fare Distribution with Outliers', fontweight='bold')\n",
        "    plt.show()\n",
        "    \n",
        "    # Decision: Keep outliers as they may represent legitimate high-fare passengers\n",
        "    # (e.g., first-class passengers who paid premium prices)\n",
        "    print(\"\\nDecision: Retaining outliers - they represent legitimate high-value passengers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Feature Engineering\n",
        "\n",
        "Creating derived features that may improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Family size\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    \n",
        "    # Is alone (family size = 1)\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "    \n",
        "    # Age groups (for potential binning)\n",
        "    data['AgeGroup'] = pd.cut(data['Age'], bins=[0, 12, 18, 35, 60, 100], \n",
        "                              labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
        "    \n",
        "    # Fare per person (accounting for family size)\n",
        "    data['FarePerPerson'] = data['Fare'] / data['FamilySize']\n",
        "    \n",
        "    # Log transform Fare (to handle skewness)\n",
        "    data['FareLog'] = np.log1p(data['Fare'])\n",
        "    \n",
        "    print(\"Feature engineering complete!\")\n",
        "    print(\"\\nNew features created:\")\n",
        "    print(\"- FamilySize: Total family members\")\n",
        "    print(\"- IsAlone: Binary indicator for solo passengers\")\n",
        "    print(\"- AgeGroup: Categorical age bins\")\n",
        "    print(\"- FarePerPerson: Fare divided by family size\")\n",
        "    print(\"- FareLog: Log-transformed fare\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Encoding\n",
        "\n",
        "Preparing categorical variables for machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None:\n",
        "    # Create encoded copy for modeling\n",
        "    data_encoded = data.copy()\n",
        "    \n",
        "    # Sex: Binary encoding (0 = male, 1 = female)\n",
        "    data_encoded['Sex'] = (data_encoded['Sex'] == 'female').astype(int)\n",
        "    \n",
        "    # Embarked: One-hot encoding (3 categories)\n",
        "    embarked_dummies = pd.get_dummies(data_encoded['Embarked'], prefix='Embarked')\n",
        "    data_encoded = pd.concat([data_encoded, embarked_dummies], axis=1)\n",
        "    data_encoded = data_encoded.drop('Embarked', axis=1)\n",
        "    \n",
        "    # Title: One-hot encoding\n",
        "    title_dummies = pd.get_dummies(data_encoded['Title'], prefix='Title')\n",
        "    data_encoded = pd.concat([data_encoded, title_dummies], axis=1)\n",
        "    data_encoded = data_encoded.drop('Title', axis=1)\n",
        "    \n",
        "    # AgeGroup: One-hot encoding\n",
        "    agegroup_dummies = pd.get_dummies(data_encoded['AgeGroup'], prefix='AgeGroup')\n",
        "    data_encoded = pd.concat([data_encoded, agegroup_dummies], axis=1)\n",
        "    data_encoded = data_encoded.drop('AgeGroup', axis=1)\n",
        "    \n",
        "    # Pclass: Keep as ordinal (1, 2, 3) - already numeric\n",
        "    \n",
        "    print(\"Categorical encoding complete!\")\n",
        "    print(f\"\\nFinal shape: {data_encoded.shape}\")\n",
        "    print(f\"\\nColumns: {list(data_encoded.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Age Distribution Before/After Imputation](images/age_imputation_comparison.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Pre/Post Cleaning Comparison\n",
        "\n",
        "Let's verify that our transformations preserved important relationships and didn't introduce data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None and 'data_encoded' in locals():\n",
        "    # Check for any remaining missing values\n",
        "    print(\"Missing Values Check:\")\n",
        "    print(\"=\"*50)\n",
        "    missing = data_encoded.isnull().sum()\n",
        "    missing = missing[missing > 0]\n",
        "    if len(missing) == 0:\n",
        "        print(\"✓ No missing values remaining!\")\n",
        "    else:\n",
        "        print(missing)\n",
        "    \n",
        "    # Verify no data leakage (target variable should not be in features)\n",
        "    if 'Survived' in data_encoded.columns:\n",
        "        print(\"\\n✓ Target variable 'Survived' is present (will be separated during modeling)\")\n",
        "    \n",
        "    # Compare Age distributions before/after imputation\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Before (with missing)\n",
        "    raw_data_with_age = raw_data['Age'].dropna()\n",
        "    axes[0].hist(raw_data_with_age, bins=30, edgecolor='black', alpha=0.7, color='#1f77b4')\n",
        "    axes[0].set_xlabel('Age (years)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Age Distribution (Before Imputation)', fontweight='bold')\n",
        "    axes[0].axvline(raw_data_with_age.median(), color='red', linestyle='--', \n",
        "                   label=f'Median: {raw_data_with_age.median():.1f}')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # After\n",
        "    axes[1].hist(data_encoded['Age'], bins=30, edgecolor='black', alpha=0.7, color='#2ca02c')\n",
        "    axes[1].set_xlabel('Age (years)')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title('Age Distribution (After Imputation)', fontweight='bold')\n",
        "    axes[1].axvline(data_encoded['Age'].median(), color='red', linestyle='--', \n",
        "                   label=f'Median: {data_encoded[\"Age\"].median():.1f}')\n",
        "    axes[1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nAge statistics comparison:\")\n",
        "    print(f\"Original (non-missing): mean={raw_data_with_age.mean():.1f}, median={raw_data_with_age.median():.1f}\")\n",
        "    print(f\"After imputation: mean={data_encoded['Age'].mean():.1f}, median={data_encoded['Age'].median():.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Final Dataset\n",
        "\n",
        "Select features for modeling and save the cleaned dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data is not None and 'data_encoded' in locals():\n",
        "    # Select features for modeling\n",
        "    # Drop PassengerId, Name, Ticket (not predictive)\n",
        "    # Keep engineered features\n",
        "    \n",
        "    feature_cols = [col for col in data_encoded.columns \n",
        "                   if col not in ['PassengerId', 'Name', 'Ticket', 'Survived']]\n",
        "    \n",
        "    X = data_encoded[feature_cols]\n",
        "    y = data_encoded['Survived']\n",
        "    \n",
        "    print(\"Final dataset prepared for modeling:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Features: {X.shape[1]}\")\n",
        "    print(f\"Samples: {X.shape[0]}\")\n",
        "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "    print(f\"\\nFeature list:\")\n",
        "    for i, col in enumerate(feature_cols, 1):\n",
        "        print(f\"{i:2d}. {col}\")\n",
        "    \n",
        "    # Save cleaned data (optional)\n",
        "    # data_encoded.to_csv('data/titanic_cleaned.csv', index=False)\n",
        "    # print(\"\\n✓ Cleaned data saved to data/titanic_cleaned.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Imputation Strategy\n",
        "- **Age**: Median imputation by Pclass and Title (preserves relationships)\n",
        "- **Embarked**: Mode imputation (only 2 missing)\n",
        "- **Fare**: Median by Pclass (1 missing)\n",
        "- **Cabin**: Converted to binary \"HasCabin\" feature (77% missing)\n",
        "\n",
        "### Feature Engineering\n",
        "- Extracted Title from Name (strong predictor)\n",
        "- Created FamilySize and IsAlone features\n",
        "- Added FarePerPerson and FareLog transformations\n",
        "- Created AgeGroup categories\n",
        "\n",
        "### Data Quality\n",
        "- ✓ No missing values remaining\n",
        "- ✓ No data leakage (target separated)\n",
        "- ✓ Outliers retained (legitimate high-fare passengers)\n",
        "- ✓ Categorical variables properly encoded\n",
        "\n",
        "### Next Steps\n",
        "1. Split data into train/test sets\n",
        "2. Standardize/normalize features for distance-based algorithms\n",
        "3. Train multiple classification models\n",
        "4. Compare performance using cross-validation\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: [Modeling →](03_modeling.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
