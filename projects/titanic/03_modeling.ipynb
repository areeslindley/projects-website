{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We'll compare multiple classification approaches to identify the strongest predictors of survival. This notebook trains and evaluates several machine learning models using cross-validation, hyperparameter tuning, and comprehensive evaluation metrics.\n",
        "\n",
        "## Navigation\n",
        "\n",
        "- **Previous**: [Data Cleaning & Imputation](02_cleaning.ipynb)\n",
        "- **Next**: [Results & Conclusions](04_results.ipynb)\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Train multiple classification models (Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, SVM)\n",
        "2. Perform hyperparameter tuning using cross-validation\n",
        "3. Evaluate models using multiple metrics (Accuracy, Precision, Recall, F1, ROC-AUC)\n",
        "4. Interpret model predictions using feature importance and SHAP values\n",
        "5. Select the best model for final evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn for modeling\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, roc_curve, confusion_matrix, classification_report)\n",
        "\n",
        "# Gradient boosting\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    print(\"XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LGBM_AVAILABLE = False\n",
        "    print(\"LightGBM not available\")\n",
        "\n",
        "# Model interpretation\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"SHAP not available\")\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_context(\"notebook\", font_scale=1.1)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "We'll recreate the cleaned dataset from the previous notebook steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data and apply cleaning steps\n",
        "# (In practice, you'd load from saved cleaned file or recreate cleaning pipeline)\n",
        "\n",
        "try:\n",
        "    raw_data = pd.read_csv('data/titanic.csv')\n",
        "    \n",
        "    # Quick cleaning (replicating key steps from 02_cleaning.ipynb)\n",
        "    data = raw_data.copy()\n",
        "    \n",
        "    # Extract Title\n",
        "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    title_mapping = {\n",
        "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
        "        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
        "        'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
        "        'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
        "        'Capt': 'Rare', 'Sir': 'Rare'\n",
        "    }\n",
        "    data['Title'] = data['Title'].map(title_mapping).fillna('Rare')\n",
        "    \n",
        "    # Impute Age\n",
        "    data['Age'] = data.groupby(['Pclass', 'Title'])['Age'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    ).fillna(data['Age'].median())\n",
        "    \n",
        "    # Impute Embarked and Fare\n",
        "    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
        "    data['Fare'] = data.groupby('Pclass')['Fare'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    \n",
        "    # Feature engineering\n",
        "    data['HasCabin'] = data['Cabin'].notna().astype(int)\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "    data['AgeGroup'] = pd.cut(data['Age'], bins=[0, 12, 18, 35, 60, 100],\n",
        "                             labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
        "    data['FarePerPerson'] = data['Fare'] / data['FamilySize']\n",
        "    data['FareLog'] = np.log1p(data['Fare'])\n",
        "    \n",
        "    # Encode categoricals\n",
        "    data['Sex'] = (data['Sex'] == 'female').astype(int)\n",
        "    embarked_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n",
        "    title_dummies = pd.get_dummies(data['Title'], prefix='Title')\n",
        "    agegroup_dummies = pd.get_dummies(data['AgeGroup'], prefix='AgeGroup')\n",
        "    data = pd.concat([data, embarked_dummies, title_dummies, agegroup_dummies], axis=1)\n",
        "    \n",
        "    # Select features\n",
        "    feature_cols = [col for col in data.columns \n",
        "                   if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', \n",
        "                                 'Embarked', 'Title', 'AgeGroup', 'Survived']]\n",
        "    \n",
        "    X = data[feature_cols]\n",
        "    y = data['Survived']\n",
        "    \n",
        "    print(f\"Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "    print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"Error: titanic.csv not found\")\n",
        "    X, y = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-Test Split\n",
        "\n",
        "We'll hold out 20% of the data for final evaluation, using stratified sampling to preserve class distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X is not None and y is not None:\n",
        "    # Stratified split to preserve class distribution\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Standardize features (important for distance-based algorithms)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"\\nTraining class distribution: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "We'll train multiple models using stratified 5-fold cross-validation. This ensures robust performance estimates while handling class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Dictionary to store results\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    \"\"\"Evaluate a model using cross-validation and test set.\"\"\"\n",
        "    # Cross-validation scores\n",
        "    cv_scores = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': [],\n",
        "        'roc_auc': []\n",
        "    }\n",
        "    \n",
        "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        \n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "        \n",
        "        cv_scores['accuracy'].append(accuracy_score(y_val, y_pred))\n",
        "        cv_scores['precision'].append(precision_score(y_val, y_pred))\n",
        "        cv_scores['recall'].append(recall_score(y_val, y_pred))\n",
        "        cv_scores['f1'].append(f1_score(y_val, y_pred))\n",
        "        cv_scores['roc_auc'].append(roc_auc_score(y_val, y_pred_proba))\n",
        "    \n",
        "    # Test set evaluation\n",
        "    model.fit(X_train, y_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    test_scores = {\n",
        "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'precision': precision_score(y_test, y_test_pred),\n",
        "        'recall': recall_score(y_test, y_test_pred),\n",
        "        'f1': f1_score(y_test, y_test_pred),\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_proba)\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'cv_mean': {k: np.mean(v) for k, v in cv_scores.items()},\n",
        "        'cv_std': {k: np.std(v) for k, v in cv_scores.items()},\n",
        "        'test': test_scores,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "print(\"Evaluation function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Logistic Regression\n",
        "\n",
        "Baseline model with interpretable coefficients. Good for understanding feature relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "    \n",
        "    lr_base = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    lr_grid = GridSearchCV(lr_base, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    lr_grid.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {lr_grid.best_params_}\")\n",
        "    print(f\"Best CV score: {lr_grid.best_score_:.4f}\")\n",
        "    \n",
        "    # Evaluate best model\n",
        "    lr_best = lr_grid.best_estimator_\n",
        "    model_results['Logistic Regression'] = evaluate_model(\n",
        "        lr_best, X_train_scaled, y_train, X_test_scaled, y_test, 'Logistic Regression'\n",
        "    )\n",
        "    trained_models['Logistic Regression'] = lr_best\n",
        "    \n",
        "    print(\"\\nLogistic Regression Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"CV Performance (mean ± std):\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        mean = model_results['Logistic Regression']['cv_mean'][metric]\n",
        "        std = model_results['Logistic Regression']['cv_std'][metric]\n",
        "        print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
        "    print(f\"\\nTest Performance:\")\n",
        "    for metric, score in model_results['Logistic Regression']['test'].items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Decision Tree\n",
        "\n",
        "Non-linear model that captures interactions. Provides feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    dt_base = DecisionTreeClassifier(random_state=42)\n",
        "    dt_grid = GridSearchCV(dt_base, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    dt_grid.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {dt_grid.best_params_}\")\n",
        "    print(f\"Best CV score: {dt_grid.best_score_:.4f}\")\n",
        "    \n",
        "    # Evaluate best model\n",
        "    dt_best = dt_grid.best_estimator_\n",
        "    model_results['Decision Tree'] = evaluate_model(\n",
        "        dt_best, X_train, y_train, X_test, y_test, 'Decision Tree'\n",
        "    )\n",
        "    trained_models['Decision Tree'] = dt_best\n",
        "    \n",
        "    print(\"\\nDecision Tree Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"CV Performance (mean ± std):\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        mean = model_results['Decision Tree']['cv_mean'][metric]\n",
        "        std = model_results['Decision Tree']['cv_std'][metric]\n",
        "        print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
        "    print(f\"\\nTest Performance:\")\n",
        "    for metric, score in model_results['Decision Tree']['test'].items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Random Forest\n",
        "\n",
        "Ensemble method that reduces overfitting and captures complex interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    # Hyperparameter tuning (using RandomizedSearchCV for speed)\n",
        "    param_dist = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [5, 10, 15, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    rf_grid = RandomizedSearchCV(rf_base, param_dist, n_iter=20, cv=cv, \n",
        "                                scoring='roc_auc', n_jobs=-1, random_state=42)\n",
        "    rf_grid.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {rf_grid.best_params_}\")\n",
        "    print(f\"Best CV score: {rf_grid.best_score_:.4f}\")\n",
        "    \n",
        "    # Evaluate best model\n",
        "    rf_best = rf_grid.best_estimator_\n",
        "    model_results['Random Forest'] = evaluate_model(\n",
        "        rf_best, X_train, y_train, X_test, y_test, 'Random Forest'\n",
        "    )\n",
        "    trained_models['Random Forest'] = rf_best\n",
        "    \n",
        "    print(\"\\nRandom Forest Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"CV Performance (mean ± std):\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        mean = model_results['Random Forest']['cv_mean'][metric]\n",
        "        std = model_results['Random Forest']['cv_std'][metric]\n",
        "        print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
        "    print(f\"\\nTest Performance:\")\n",
        "    for metric, score in model_results['Random Forest']['test'].items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Gradient Boosting (XGBoost)\n",
        "\n",
        "State-of-the-art performance for tabular data. Handles non-linear relationships effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_train is not None and XGB_AVAILABLE:\n",
        "    # Hyperparameter tuning\n",
        "    param_dist = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    }\n",
        "    \n",
        "    xgb_base = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "    xgb_grid = RandomizedSearchCV(xgb_base, param_dist, n_iter=15, cv=cv,\n",
        "                                  scoring='roc_auc', n_jobs=-1, random_state=42)\n",
        "    xgb_grid.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {xgb_grid.best_params_}\")\n",
        "    print(f\"Best CV score: {xgb_grid.best_score_:.4f}\")\n",
        "    \n",
        "    # Evaluate best model\n",
        "    xgb_best = xgb_grid.best_estimator_\n",
        "    model_results['XGBoost'] = evaluate_model(\n",
        "        xgb_best, X_train, y_train, X_test, y_test, 'XGBoost'\n",
        "    )\n",
        "    trained_models['XGBoost'] = xgb_best\n",
        "    \n",
        "    print(\"\\nXGBoost Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"CV Performance (mean ± std):\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        mean = model_results['XGBoost']['cv_mean'][metric]\n",
        "        std = model_results['XGBoost']['cv_std'][metric]\n",
        "        print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
        "    print(f\"\\nTest Performance:\")\n",
        "    for metric, score in model_results['XGBoost']['test'].items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")\n",
        "else:\n",
        "    print(\"XGBoost not available - skipping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Support Vector Machine\n",
        "\n",
        "Different decision boundary approach, useful for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    # Hyperparameter tuning (using smaller grid due to computational cost)\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "    \n",
        "    svm_base = SVC(probability=True, random_state=42)\n",
        "    svm_grid = GridSearchCV(svm_base, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)  # Reduced CV folds\n",
        "    svm_grid.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    print(f\"Best parameters: {svm_grid.best_params_}\")\n",
        "    print(f\"Best CV score: {svm_grid.best_score_:.4f}\")\n",
        "    \n",
        "    # Evaluate best model\n",
        "    svm_best = svm_grid.best_estimator_\n",
        "    model_results['SVM'] = evaluate_model(\n",
        "        svm_best, X_train_scaled, y_train, X_test_scaled, y_test, 'SVM'\n",
        "    )\n",
        "    trained_models['SVM'] = svm_best\n",
        "    \n",
        "    print(\"\\nSVM Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"CV Performance (mean ± std):\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "        mean = model_results['SVM']['cv_mean'][metric]\n",
        "        std = model_results['SVM']['cv_std'][metric]\n",
        "        print(f\"  {metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
        "    print(f\"\\nTest Performance:\")\n",
        "    for metric, score in model_results['SVM']['test'].items():\n",
        "        print(f\"  {metric.capitalize()}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison Summary\n",
        "\n",
        "Let's create a comprehensive comparison table of all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(model_results) > 0:\n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    for model_name, results in model_results.items():\n",
        "        row = {'Model': model_name}\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
        "            mean = results['cv_mean'][metric]\n",
        "            std = results['cv_std'][metric]\n",
        "            row[f'{metric.capitalize()} (CV)'] = f\"{mean:.4f} ± {std:.4f}\"\n",
        "            row[f'{metric.capitalize()} (Test)'] = f\"{results['test'][metric]:.4f}\"\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    display(comparison_df)\n",
        "    \n",
        "    # Visualize ROC-AUC comparison\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    models = list(model_results.keys())\n",
        "    cv_auc = [model_results[m]['cv_mean']['roc_auc'] for m in models]\n",
        "    cv_std = [model_results[m]['cv_std']['roc_auc'] for m in models]\n",
        "    test_auc = [model_results[m]['test']['roc_auc'] for m in models]\n",
        "    \n",
        "    x = np.arange(len(models))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax.bar(x - width/2, cv_auc, width, yerr=cv_std, label='CV (mean ± std)', alpha=0.8)\n",
        "    ax.bar(x + width/2, test_auc, width, label='Test', alpha=0.8)\n",
        "    \n",
        "    ax.set_ylabel('ROC-AUC Score')\n",
        "    ax.set_title('Model Comparison: ROC-AUC', fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Feature Importance](images/feature_importance.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "Understanding which features drive predictions helps interpret model behavior and validate domain knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(trained_models) > 0:\n",
        "    # Feature importance for tree-based models\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    plot_idx = 0\n",
        "    \n",
        "    # Random Forest\n",
        "    if 'Random Forest' in trained_models:\n",
        "        rf_model = trained_models['Random Forest']\n",
        "        importances = rf_model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1][:15]  # Top 15\n",
        "        \n",
        "        axes[plot_idx].barh(range(len(indices)), importances[indices])\n",
        "        axes[plot_idx].set_yticks(range(len(indices)))\n",
        "        axes[plot_idx].set_yticklabels([X.columns[i] for i in indices])\n",
        "        axes[plot_idx].set_xlabel('Importance')\n",
        "        axes[plot_idx].set_title('Random Forest: Top 15 Features', fontweight='bold')\n",
        "        axes[plot_idx].invert_yaxis()\n",
        "        plot_idx += 1\n",
        "    \n",
        "    # Decision Tree\n",
        "    if 'Decision Tree' in trained_models:\n",
        "        dt_model = trained_models['Decision Tree']\n",
        "        importances = dt_model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1][:15]\n",
        "        \n",
        "        axes[plot_idx].barh(range(len(indices)), importances[indices])\n",
        "        axes[plot_idx].set_yticks(range(len(indices)))\n",
        "        axes[plot_idx].set_yticklabels([X.columns[i] for i in indices])\n",
        "        axes[plot_idx].set_xlabel('Importance')\n",
        "        axes[plot_idx].set_title('Decision Tree: Top 15 Features', fontweight='bold')\n",
        "        axes[plot_idx].invert_yaxis()\n",
        "        plot_idx += 1\n",
        "    \n",
        "    # XGBoost\n",
        "    if 'XGBoost' in trained_models:\n",
        "        xgb_model = trained_models['XGBoost']\n",
        "        importances = xgb_model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1][:15]\n",
        "        \n",
        "        axes[plot_idx].barh(range(len(indices)), importances[indices])\n",
        "        axes[plot_idx].set_yticks(range(len(indices)))\n",
        "        axes[plot_idx].set_yticklabels([X.columns[i] for i in indices])\n",
        "        axes[plot_idx].set_xlabel('Importance')\n",
        "        axes[plot_idx].set_title('XGBoost: Top 15 Features', fontweight='bold')\n",
        "        axes[plot_idx].invert_yaxis()\n",
        "        plot_idx += 1\n",
        "    \n",
        "    # Logistic Regression coefficients\n",
        "    if 'Logistic Regression' in trained_models:\n",
        "        lr_model = trained_models['Logistic Regression']\n",
        "        coef = lr_model.coef_[0]\n",
        "        indices = np.argsort(np.abs(coef))[::-1][:15]\n",
        "        \n",
        "        axes[plot_idx].barh(range(len(indices)), coef[indices])\n",
        "        axes[plot_idx].set_yticks(range(len(indices)))\n",
        "        axes[plot_idx].set_yticklabels([X.columns[i] for i in indices])\n",
        "        axes[plot_idx].set_xlabel('Coefficient')\n",
        "        axes[plot_idx].set_title('Logistic Regression: Top 15 Coefficients', fontweight='bold')\n",
        "        axes[plot_idx].invert_yaxis()\n",
        "        axes[plot_idx].axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(plot_idx + 1, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SHAP_AVAILABLE and 'Random Forest' in trained_models:\n",
        "    # Compute SHAP values for Random Forest (sample for speed)\n",
        "    rf_model = trained_models['Random Forest']\n",
        "    \n",
        "    # Use a subset for SHAP computation (can be slow)\n",
        "    X_sample = X_train.iloc[:100]  # Sample 100 instances\n",
        "    \n",
        "    explainer = shap.TreeExplainer(rf_model)\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "    \n",
        "    # Summary plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
        "    plt.title('SHAP Feature Importance (Random Forest)', fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"SHAP values computed for Random Forest model\")\n",
        "    print(\"(Positive SHAP values increase survival probability, negative decrease it)\")\n",
        "else:\n",
        "    print(\"SHAP not available or Random Forest not trained - skipping SHAP analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Models\n",
        "\n",
        "Save the best models for use in the results notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models and scaler\n",
        "if len(trained_models) > 0:\n",
        "    # Create directory if needed\n",
        "    import os\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    \n",
        "    # Save each model\n",
        "    for name, model in trained_models.items():\n",
        "        filename = f\"models/{name.lower().replace(' ', '_')}.pkl\"\n",
        "        joblib.dump(model, filename)\n",
        "        print(f\"Saved: {filename}\")\n",
        "    \n",
        "    # Save scaler\n",
        "    if 'scaler' in locals():\n",
        "        joblib.dump(scaler, 'models/scaler.pkl')\n",
        "        print(\"Saved: models/scaler.pkl\")\n",
        "    \n",
        "    # Save model results\n",
        "    import json\n",
        "    results_summary = {}\n",
        "    for name, results in model_results.items():\n",
        "        results_summary[name] = {\n",
        "            'cv_mean': results['cv_mean'],\n",
        "            'test': results['test']\n",
        "        }\n",
        "    \n",
        "    with open('models/results_summary.json', 'w') as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "    print(\"Saved: models/results_summary.json\")\n",
        "    \n",
        "    print(\"\\n✓ All models and results saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Model Performance\n",
        "- All models achieved reasonable performance (ROC-AUC > 0.80)\n",
        "- Ensemble methods (Random Forest, XGBoost) generally performed best\n",
        "- Cross-validation and test set performance were consistent (no severe overfitting)\n",
        "\n",
        "### Feature Importance\n",
        "- **Sex** and **Pclass** consistently ranked as top predictors (validates domain knowledge)\n",
        "- **Title** and **Fare** also showed strong predictive power\n",
        "- Family-related features (FamilySize, IsAlone) had moderate importance\n",
        "\n",
        "### Model Selection\n",
        "- **Random Forest** or **XGBoost** appear to be the best choices for final deployment\n",
        "- **Logistic Regression** provides good interpretability with competitive performance\n",
        "- All models capture the \"women and children first\" pattern (Sex, Title, Age effects)\n",
        "\n",
        "### Next Steps\n",
        "1. Detailed model comparison with statistical significance tests\n",
        "2. ROC and Precision-Recall curve visualization\n",
        "3. Error analysis on misclassified cases\n",
        "4. Final model selection and deployment recommendations\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: [Results & Conclusions →](04_results.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
