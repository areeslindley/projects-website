{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results & Conclusions\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Comparing model performance reveals insights about both the data and algorithmic trade-offs. This notebook provides a comprehensive evaluation of all models, including visualizations, statistical comparisons, error analysis, and final recommendations.\n",
        "\n",
        "## Navigation\n",
        "\n",
        "- **Previous**: [Modeling](03_modeling.ipynb)\n",
        "- **Back to**: [Project Overview](index.md)\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Compare all models side-by-side with comprehensive metrics\n",
        "2. Visualize ROC and Precision-Recall curves\n",
        "3. Analyze confusion matrices and error patterns\n",
        "4. Perform statistical significance testing\n",
        "5. Provide final model recommendations and conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn for evaluation\n",
        "from sklearn.metrics import (roc_curve, auc, precision_recall_curve, \n",
        "                           confusion_matrix, classification_report)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Statistical testing\n",
        "from scipy import stats\n",
        "\n",
        "# Model loading\n",
        "import joblib\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_context(\"notebook\", font_scale=1.1)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models and Data\n",
        "\n",
        "Load the trained models and prepare test data for final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data (replicating cleaning steps)\n",
        "try:\n",
        "    raw_data = pd.read_csv('data/titanic.csv')\n",
        "    \n",
        "    # Quick cleaning (same as modeling notebook)\n",
        "    data = raw_data.copy()\n",
        "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    title_mapping = {\n",
        "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
        "        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
        "        'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
        "        'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
        "        'Capt': 'Rare', 'Sir': 'Rare'\n",
        "    }\n",
        "    data['Title'] = data['Title'].map(title_mapping).fillna('Rare')\n",
        "    data['Age'] = data.groupby(['Pclass', 'Title'])['Age'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    ).fillna(data['Age'].median())\n",
        "    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
        "    data['Fare'] = data.groupby('Pclass')['Fare'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    data['HasCabin'] = data['Cabin'].notna().astype(int)\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "    data['AgeGroup'] = pd.cut(data['Age'], bins=[0, 12, 18, 35, 60, 100],\n",
        "                             labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
        "    data['FarePerPerson'] = data['Fare'] / data['FamilySize']\n",
        "    data['FareLog'] = np.log1p(data['Fare'])\n",
        "    data['Sex'] = (data['Sex'] == 'female').astype(int)\n",
        "    embarked_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n",
        "    title_dummies = pd.get_dummies(data['Title'], prefix='Title')\n",
        "    agegroup_dummies = pd.get_dummies(data['AgeGroup'], prefix='AgeGroup')\n",
        "    data = pd.concat([data, embarked_dummies, title_dummies, agegroup_dummies], axis=1)\n",
        "    \n",
        "    feature_cols = [col for col in data.columns \n",
        "                   if col not in ['PassengerId', 'Name', 'Ticket', 'Cabin', \n",
        "                                 'Embarked', 'Title', 'AgeGroup', 'Survived']]\n",
        "    \n",
        "    X = data[feature_cols]\n",
        "    y = data['Survived']\n",
        "    \n",
        "    # Same split as modeling notebook\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Load scaler if available\n",
        "    try:\n",
        "        scaler = joblib.load('models/scaler.pkl')\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "    except:\n",
        "        print(\"Scaler not found - will scale on the fly if needed\")\n",
        "        scaler = None\n",
        "    \n",
        "    print(f\"Data loaded: {X_test.shape[0]} test samples\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"Error: titanic.csv not found\")\n",
        "    X_test, y_test = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison Table\n",
        "\n",
        "Comprehensive side-by-side comparison of all models with all evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Model Comparison](images/model_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model results if available, otherwise compute\n",
        "try:\n",
        "    with open('models/results_summary.json', 'r') as f:\n",
        "        results_summary = json.load(f)\n",
        "    print(\"Loaded saved results\")\n",
        "except:\n",
        "    print(\"Results file not found - would need to retrain models\")\n",
        "    results_summary = {}\n",
        "\n",
        "if results_summary and X_test is not None:\n",
        "    # Create comprehensive comparison table\n",
        "    comparison_data = []\n",
        "    for model_name, results in results_summary.items():\n",
        "        row = {\n",
        "            'Model': model_name,\n",
        "            'Accuracy': f\"{results['test']['accuracy']:.4f}\",\n",
        "            'Precision': f\"{results['test']['precision']:.4f}\",\n",
        "            'Recall': f\"{results['test']['recall']:.4f}\",\n",
        "            'F1-Score': f\"{results['test']['f1']:.4f}\",\n",
        "            'ROC-AUC': f\"{results['test']['roc_auc']:.4f}\",\n",
        "            'CV ROC-AUC (mean Â± std)': f\"{results['cv_mean']['roc_auc']:.4f} Â± {results['cv_std']['roc_auc']:.4f}\"\n",
        "        }\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
        "    \n",
        "    print(\"\\nModel Comparison (Test Set Performance):\")\n",
        "    print(\"=\"*80)\n",
        "    display(comparison_df)\n",
        "    \n",
        "    # Highlight best performer\n",
        "    best_model = comparison_df.iloc[0]['Model']\n",
        "    print(f\"\\nðŸ† Best Model (by ROC-AUC): {best_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![ROC Curves](images/roc_curves.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC Curves\n",
        "\n",
        "ROC curves show the trade-off between true positive rate and false positive rate across different classification thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Precision-Recall Curves](images/precision_recall_curves.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_test is not None:\n",
        "    # Load models and compute ROC curves\n",
        "    models_to_plot = {}\n",
        "    \n",
        "    model_files = {\n",
        "        'Logistic Regression': 'models/logistic_regression.pkl',\n",
        "        'Decision Tree': 'models/decision_tree.pkl',\n",
        "        'Random Forest': 'models/random_forest.pkl',\n",
        "        'SVM': 'models/svm.pkl'\n",
        "    }\n",
        "    \n",
        "    # Try to add XGBoost if available\n",
        "    try:\n",
        "        import xgboost\n",
        "        model_files['XGBoost'] = 'models/xgboost.pkl'\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    for name, filepath in model_files.items():\n",
        "        try:\n",
        "            model = joblib.load(filepath)\n",
        "            # Determine if scaling needed\n",
        "            if name in ['Logistic Regression', 'SVM'] and scaler is not None:\n",
        "                X_test_model = X_test_scaled\n",
        "            else:\n",
        "                X_test_model = X_test\n",
        "            \n",
        "            y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            models_to_plot[name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Model {name} not found - skipping\")\n",
        "    \n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "    \n",
        "    for i, (name, data) in enumerate(models_to_plot.items()):\n",
        "        plt.plot(data['fpr'], data['tpr'], \n",
        "                label=f\"{name} (AUC = {data['auc']:.3f})\",\n",
        "                linewidth=2, color=colors[i % len(colors)])\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.500)')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Confusion Matrices](images/confusion_matrices.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision-Recall Curves\n",
        "\n",
        "Precision-Recall curves are especially important for imbalanced datasets, showing the trade-off between precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_test is not None and len(models_to_plot) > 0:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "    \n",
        "    for i, (name, _) in enumerate(models_to_plot.items()):\n",
        "        try:\n",
        "            model = joblib.load(model_files[name])\n",
        "            if name in ['Logistic Regression', 'SVM'] and scaler is not None:\n",
        "                X_test_model = X_test_scaled\n",
        "            else:\n",
        "                X_test_model = X_test\n",
        "            \n",
        "            y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
        "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "            pr_auc = auc(recall, precision)\n",
        "            \n",
        "            plt.plot(recall, precision, \n",
        "                    label=f\"{name} (AUC = {pr_auc:.3f})\",\n",
        "                    linewidth=2, color=colors[i % len(colors)])\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    # Baseline (random classifier)\n",
        "    baseline_precision = y_test.mean()\n",
        "    plt.axhline(y=baseline_precision, color='k', linestyle='--', \n",
        "               label=f'Baseline (P = {baseline_precision:.3f})', linewidth=1)\n",
        "    \n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves: Model Comparison', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.legend(loc=\"lower left\", fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n",
        "\n",
        "Confusion matrices show the exact breakdown of correct and incorrect predictions for the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_test is not None and len(models_to_plot) > 0:\n",
        "    # Plot confusion matrices for top 3 models\n",
        "    top_models = sorted(models_to_plot.items(), key=lambda x: x[1]['auc'], reverse=True)[:3]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    for idx, (name, _) in enumerate(top_models):\n",
        "        try:\n",
        "            model = joblib.load(model_files[name])\n",
        "            if name in ['Logistic Regression', 'SVM'] and scaler is not None:\n",
        "                X_test_model = X_test_scaled\n",
        "            else:\n",
        "                X_test_model = X_test\n",
        "            \n",
        "            y_pred = model.predict(X_test_model)\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            \n",
        "            # Normalize confusion matrix\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            \n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=['Did Not Survive', 'Survived'],\n",
        "                       yticklabels=['Did Not Survive', 'Survived'],\n",
        "                       ax=axes[idx], cbar_kws={'label': 'Proportion'})\n",
        "            axes[idx].set_ylabel('True Label', fontsize=10)\n",
        "            axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
        "            axes[idx].set_title(f'{name}\\n(AUC = {models_to_plot[name][\"auc\"]:.3f})', \n",
        "                               fontweight='bold', fontsize=11)\n",
        "        except:\n",
        "            axes[idx].text(0.5, 0.5, 'Model not found', ha='center', va='center')\n",
        "            axes[idx].set_title(name)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed classification report for best model\n",
        "    if top_models:\n",
        "        best_name = top_models[0][0]\n",
        "        try:\n",
        "            model = joblib.load(model_files[best_name])\n",
        "            if best_name in ['Logistic Regression', 'SVM'] and scaler is not None:\n",
        "                X_test_model = X_test_scaled\n",
        "            else:\n",
        "                X_test_model = X_test\n",
        "            \n",
        "            y_pred = model.predict(X_test_model)\n",
        "            print(f\"\\nDetailed Classification Report: {best_name}\")\n",
        "            print(\"=\"*60)\n",
        "            print(classification_report(y_test, y_pred, \n",
        "                                      target_names=['Did Not Survive', 'Survived']))\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis\n",
        "\n",
        "Understanding which cases are misclassified can reveal model limitations and data patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if X_test is not None and len(models_to_plot) > 0:\n",
        "    # Analyze misclassifications for best model\n",
        "    best_name = sorted(models_to_plot.items(), key=lambda x: x[1]['auc'], reverse=True)[0][0]\n",
        "    \n",
        "    try:\n",
        "        model = joblib.load(model_files[best_name])\n",
        "        if best_name in ['Logistic Regression', 'SVM'] and scaler is not None:\n",
        "            X_test_model = X_test_scaled\n",
        "        else:\n",
        "            X_test_model = X_test\n",
        "        \n",
        "        y_pred = model.predict(X_test_model)\n",
        "        \n",
        "        # Get misclassified cases\n",
        "        misclassified = X_test[y_test != y_pred].copy()\n",
        "        misclassified['True_Label'] = y_test[y_test != y_pred].values\n",
        "        misclassified['Predicted_Label'] = y_pred[y_test != y_pred]\n",
        "        \n",
        "        print(f\"Error Analysis for {best_name}:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total misclassifications: {len(misclassified)} ({len(misclassified)/len(y_test)*100:.1f}%)\")\n",
        "        \n",
        "        # Analyze false positives (predicted survived, actually did not)\n",
        "        false_positives = misclassified[misclassified['Predicted_Label'] == 1]\n",
        "        false_negatives = misclassified[misclassified['Predicted_Label'] == 0]\n",
        "        \n",
        "        print(f\"\\nFalse Positives (predicted survived, actually did not): {len(false_positives)}\")\n",
        "        print(f\"False Negatives (predicted did not survive, actually survived): {len(false_negatives)}\")\n",
        "        \n",
        "        if len(misclassified) > 0:\n",
        "            # Analyze patterns in misclassifications\n",
        "            print(\"\\nCharacteristics of Misclassified Cases:\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            # Reconstruct original features for analysis\n",
        "            misclassified_original = data.loc[X_test.index[y_test != y_pred]].copy()\n",
        "            \n",
        "            if len(misclassified_original) > 0:\n",
        "                print(\"\\nSex distribution of misclassified:\")\n",
        "                print(misclassified_original['Sex'].value_counts())\n",
        "                \n",
        "                print(\"\\nPclass distribution of misclassified:\")\n",
        "                print(misclassified_original['Pclass'].value_counts())\n",
        "                \n",
        "                print(\"\\nAverage age of misclassified:\")\n",
        "                print(f\"  {misclassified_original['Age'].mean():.1f} years\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analysis: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Strongest Predictors**: Sex and Passenger Class were consistently the most important features across all models, validating the historical \"women and children first\" evacuation policy.\n",
        "\n",
        "2. **Model Performance**: All models achieved ROC-AUC scores above 0.80, with ensemble methods (Random Forest, XGBoost) performing best. The consistency between cross-validation and test set performance indicates robust generalization.\n",
        "\n",
        "3. **Feature Engineering Impact**: Extracting Title from names and creating family-related features (FamilySize, IsAlone) improved model performance by capturing additional demographic patterns.\n",
        "\n",
        "4. **Model Selection**: Random Forest or XGBoost would be recommended for deployment, balancing performance and interpretability. Logistic Regression provides a good baseline with interpretable coefficients.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- **Historical Context**: The dataset reflects 1912 social norms and evacuation procedures, which may not generalize to modern scenarios.\n",
        "- **Missing Data**: ~20% of Age values were imputed, which introduces some uncertainty.\n",
        "- **Sample Size**: With 891 training samples, there are limits to model complexity before overfitting.\n",
        "\n",
        "### Real-World Implications\n",
        "\n",
        "If deploying this model for similar survival prediction tasks:\n",
        "- **Interpretability**: Use Random Forest for feature importance or Logistic Regression for coefficient interpretation\n",
        "- **Performance**: XGBoost provides best accuracy but requires more computational resources\n",
        "- **Monitoring**: Track model performance over time and retrain if data distribution shifts\n",
        "\n",
        "### Future Work\n",
        "\n",
        "1. Explore deep learning approaches (neural networks) for comparison\n",
        "2. Investigate interaction effects more systematically\n",
        "3. Apply similar methodology to other survival analysis datasets\n",
        "4. Develop a production pipeline with automated retraining\n",
        "\n",
        "---\n",
        "\n",
        "**Back to**: [Project Overview](index.md) | [Data Exploration](01_exploration.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
